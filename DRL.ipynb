{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table>\n",
    "    <tr>\n",
    "        <td>\n",
    "            <img src='./text_images/nvidia.png' width=\"200\" height=\"450\">\n",
    "        </td>\n",
    "        <td> & </td>\n",
    "        <td>\n",
    "            <img src='./text_images/udacity.png' width=\"350\" height=\"450\">\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Reinforcement Learning for Optimal Execution of Portfolio Transactions \n",
    "用于执行最佳投资组合交易的深度强化学习\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "介绍\n",
    "\n",
    "This notebook demonstrates how to use Deep Reinforcement Learning (DRL) for optimizing the execution of large portfolio transactions. We begin with a brief review of reinforcement learning and actor-critic methods.  Then, you will use an actor-critic method to generate optimal trading strategies that maximize profit when liquidating a block of shares.   \n",
    "本笔记本演示了如何使用深度强化学习（DRL）来优化大型投资组合交易的执行。我们首先简要回顾一下强化学习和演员批评家方法。然后，您将使用演员批评家方法来生成最佳交易策略，以在清算股份时最大化利润。\n",
    "\n",
    "# Actor-Critic Methods\n",
    "演员批评家方法\n",
    "\n",
    "In reinforcement learning, an agent makes observations and takes actions within an environment, and in return it receives rewards. Its objective is to learn to act in a way that will maximize its expected long-term rewards.   \n",
    "在强化学习中，代理在环境中进行观察并采取行动，作为回报，它会获得奖励。 它的目标是学习以最大程度地获得其预期长期回报的方式行事。\n",
    "\n",
    "<br>\n",
    "<figure>\n",
    "  <img src = \"./text_images/RL.png\" width = 80% style = \"border: thin silver solid; padding: 10px\">\n",
    "      <figcaption style = \"text-align: center; font-style: italic\">Fig 1. - Reinforcement Learning.</figcaption>\n",
    "</figure> \n",
    "<br>\n",
    "\n",
    "There are several types of RL algorithms, and they can be divided into three groups:  \n",
    "强化学习算法有几种类型，可以分为三类：\n",
    "\n",
    "- **Critic-Only**: Critic-Only methods, also known as Value-Based methods, first find the optimal value function and then derive an optimal policy from it.   \n",
    "仅限批评家：仅限批评家方法，也称为基于价值的方法，首先找到最优值函数然后从中得到最优策略  \n",
    "\n",
    "\n",
    "- **Actor-Only**: Actor-Only methods, also known as Policy-Based methods, search directly for the optimal policy in policy space. This is typically done by using a parameterized family of policies over which optimization procedures can be used directly.   \n",
    "仅限演员：仅限演员的方法，也称为基于策略的方法，直接在策略空间中搜索最佳策略。通常，这是通过使用参数化的策略系列来完成的，可以直接使用优化过程。  \n",
    "\n",
    "\n",
    "- **Actor-Critic**: Actor-Critic methods combine the advantages of actor-only and critic-only methods. In this method, the critic learns the value function and uses it to determine how the actor's policy parameters should be changed. In this case, the actor brings the advantage of computing continuous actions without the need for optimization procedures on a value function, while the critic supplies the actor with knowledge of the performance. Actor-critic methods usually have good convergence properties, in contrast to critic-only methods.  The **Deep Deterministic Policy Gradients (DDPG)** algorithm is one example of an actor-critic method.  \n",
    "演员-批评家：演员-批评家方法结合了仅限演员和仅限批评家方法的优点。在这个方法里面，评论家学习了价值函数，并用他来决定演员的策略参数如何调整。在这种情况下，演员带来了计算连续性动作的优势而无需对价值函数进行优化程序，而批评家为演员提供了性能方面的知识。演员-批评家方法通常具有很好的收敛性，这与仅限批评家方法正好相反。深度确定性策略梯度(DDPG)方法就是一个演员批评家的示例\n",
    "\n",
    "<br>\n",
    "<figure>\n",
    "  <img src = \"./text_images/Actor-Critic.png\" width = 80% style = \"border: thin silver solid; padding: 10px\">\n",
    "      <figcaption style = \"text-align: center; font-style: italic\">Fig 2. - Actor-Critic Reinforcement Learning.</figcaption>\n",
    "</figure> \n",
    "<br>\n",
    "\n",
    "In this notebook, we will use DDPG to determine the optimal execution of portfolio transactions. In other words, we will use the DDPG algorithm to solve the optimal liquidation problem. But before we can apply the DDPG algorithm we first need to formulate the optimal liquidation problem so that in can be solved using reinforcement learning. In the next section we will see how to do this.   \n",
    "在本笔记本中，我们将使用深度确定性策略梯度DDPG去决定投资组合交易的最佳执行。换句话说，我们将使用DDPG算法去解决最佳清算问题。但是在我们应用DDPG算法之前，我们首先需要用公式表达最优清算问题，以便可以使用强化学习来解决。在下一节中，我们将看到如何执行此操作"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling Optimal Execution as a Reinforcement Learning Problem\n",
    "将最佳执行问题建模为强化学习问题\n",
    "\n",
    "As we learned in the previous lessons, the optimal liquidation problem is a minimization problem, *i.e.* we need to find the trading list that minimizes the implementation shortfall. In order to solve this problem through reinforcement learning, we need to restate the optimal liquidation problem in terms of **States**, **Actions**, and **Rewards**. Let's start by defining our States.  \n",
    "正如我们前面课程锁学习的那样，最优清算问题是一个最小化问题，即，我们需要找到一个能够最大程度减少实施缺口的交易清单。为了通过强化学习来解决这个问题，我们需要根据“状态”、“动作”和“奖励”来重新陈述最优清算问题。让我们从定义状态开始\n",
    "\n",
    "### States\n",
    "状态\n",
    "\n",
    "The optimal liquidation problem entails that we sell all our shares within a given time frame. Therefore, our state vector must contain some information about the time remaining, or what is equivalent, the number trades remaning. We will use the latter and use the following features to define the state vector at time $t_k$:  \n",
    "最优清算问题需要我们在一个给定的时间内出售所有股票。因此，我们的状态向量必须包含一些关于剩余时间或者剩余数量的信息。我们将用后者，并使用以下特征去定义一个在时间$t_k$的状态向量    \n",
    "\n",
    "\n",
    "$$\n",
    "[r_{k-5},\\, r_{k-4},\\, r_{k-3},\\, r_{k-2},\\, r_{k-1},\\, r_{k},\\, m_{k},\\, i_{k}]\n",
    "$$\n",
    "\n",
    "where:    \n",
    "所以：  \n",
    "\n",
    "- $r_{k} = \\log\\left(\\frac{\\tilde{S}_k}{\\tilde{S}_{k-1}}\\right)$ is the log-return at time $t_k$  \n",
    "- $r_{k} = \\log\\left(\\frac{\\tilde{S}_k}{\\tilde{S}_{k-1}}\\right)$  是在时间$t_k$的对数返回值    \n",
    "\n",
    "\n",
    "- $m_{k} = \\frac{N_k}{N}$ is the number of trades remaining at time $t_k$ normalized by the total number of trades.  \n",
    "- $m_{k} = \\frac{N_k}{N}$ 是在时间$t_k$通过交易总数归一化之后的剩余交易数量  \n",
    "\n",
    "\n",
    "- $i_{k} = \\frac{x_k}{X}$ is the remaining number of shares at time $t_k$ normalized by the total number of shares.  \n",
    "- $i_{k} = \\frac{x_k}{X}$ 是在时间$t_k$通过股份总数归一化之后的剩余股份数量\n",
    "\n",
    "\n",
    "The log-returns capture information about stock prices before time $t_k$, which can be used to detect possible price trends. The number of trades and shares remaining allow the agent to learn to sell all the shares within a given time frame. It is important to note that in real world trading scenarios, this state vector can hold many more variables.   \n",
    "对数返回捕捉时间$t_k$之前的股票价格信息，可以用来检测可能的价格趋势。剩余的交易数量和股票数量允许智能体去学习在给定的时间内出售所有股票。需要注意的是，在现实世界的交易场景中，状态向量可以容纳更多变量  \n",
    "\n",
    "### Actions\n",
    "动作  \n",
    "\n",
    "Since the optimal liquidation problem only requires us to sell stocks, it is reasonable to define the action $a_k$ to be the number of shares to sell at time $t_{k}$. However, if we start with millions of stocks, intepreting the action directly as the number of shares to sell at each time step can lead to convergence problems, because, the agent will need to produce actions with very high values. Instead, we will interpret the action $a_k$ as a **percentage**. In this case, the actions produced by the agent will only need to be between 0 and 1. Using this interpretation, we can determine the number of shares to sell at each time step using:  \n",
    "由于最优清算问题仅需要我们去出售股票，因此将动作$a_k$定义为在时间$t_{k}$时要出售的股票数量是合理的。但是，如果我们从数以百万计的股票开始，则将动作直接解释为在每个时间步骤出售的股票数量将会导致收敛的问题，因为智能体将需要相当高的代价去产生动作。相反，我们将动作$a_k$解释为百分比。在这种情况下，智能体产生动作只需要介于0和1之间。使用这个表示方法，我们可以使用以下方法确定每个时间步骤出售的股票数量：  \n",
    "\n",
    "\n",
    "$$\n",
    "n_k = a_k \\times x_k\n",
    "$$\n",
    "\n",
    "where $x_k$ is the number of shares remaining at time $t_k$.    \n",
    "其中$x_k$是在时间$t_k$的剩余的股票数量\n",
    "\n",
    "### Rewards\n",
    "奖励\n",
    "\n",
    "Defining the rewards is trickier than defining states and actions, since the original problem is a minimization problem. One option is to use the difference between two consecutive utility functions. Remeber the utility function is given by:  \n",
    "定义奖励比定义状态和动作要复杂得多，因为原始问题是最小化问题。 一种选择是利用两个连续的效用函数的差异。 记住效用函数如下：  \n",
    "\n",
    "$$\n",
    "U(x) = E(x) + λ V(x)\n",
    "$$\n",
    "\n",
    "After each time step, we compute the utility using the equations for $E(x)$ and $V(x)$ from the Almgren and Chriss model for the remaining time and inventory while holding parameter λ constant. Denoting the optimal trading trajectory computed at time $t$ as $x^*_t$, we define the reward as:   \n",
    "在每一个时间段,我们利用等Almgren和Chriss的模型中的$E(x)$和$V(x)$的方程式计算剩余时间和库存量的效用，并同时保持参数λ不变。将在时间$t$最佳交易轨迹表示为$x^*_t$，我们将奖励定义为如下：  \n",
    "\n",
    "$$\n",
    "R_{t} = {{U_t(x^*_t) - U_{t+1}(x^*_{t+1})}\\over{U_t(x^*_t)}}\n",
    "$$\n",
    "\n",
    "Where we have normalized the difference to train the actor-critic model easier.  \n",
    "我们已将差异归一化，以便更轻松的训练演员-批判家模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simulation Environment\n",
    "模拟环境\n",
    "\n",
    "In order to train our DDPG algorithm we will use a very simple simulated trading environment. This environment simulates stock prices that follow a discrete arithmetic random walk and that the permanent and temporary market impact functions are linear functions of the rate of trading, just like in the Almgren and Chriss model. This simple trading environment serves as a starting point to create more complex trading environments. You are encouraged to extend this simple trading environment by adding more complexity to simulte real world trading dynamics, such as book orders, network latencies, trading fees, etc...   \n",
    "为了训练我们的DDPG算法，我们将使用一个非常简单的模拟交易环境。这种环境模拟了遵循离线算术随机游动的的股票价格，并且永久性和临时性市场影响函数是交易率的的线性函数，就像在Almgren和Chriss模型中的一样。这个简单的交易环境是创建更复杂的交易环境的起点。我们鼓励你通过增加更多复杂性来模拟现实世界中的交易动态来扩展这种简单的交易环境，例如订单，网络等待时间，交易费用等。\n",
    "\n",
    "The simulated enviroment is contained in the **syntheticChrissAlmgren.py** module. You are encouraged to take a look it and modify its parameters as you wish. Let's take a look at the default parameters of our simulation environment. We have set the intial stock price to be $S_0 = 50$, and the total number of shares to sell to one million. This gives an initial portfolio value of $\\$50$ Million dollars. We have also set the trader's risk aversion to $\\lambda = 10^{-6}$.  \n",
    "模拟的环境包含在**syntheticChrissAlmgren.py**模块中。建议你看一下并根据需要修改其参数。让我们看一下仿真环境的默认参数。我们将初始股票价格设置为$S_0 = 50$，要出售的股票总数是100万。这样得出初始投资组合值为$\\$50$百万美元。我们还将交易者的风险规避设置为$\\lambda = 10^{-6}$\n",
    "\n",
    "The stock price will have 12\\% annual volatility, a [bid-ask spread](https://www.investopedia.com/terms/b/bid-askspread.asp) of 1/8 and an average daily trading volume of 5 million shares. Assuming there are 250 trading days in a year, this gives a daily volatility in stock price of $0.12 / \\sqrt{250} \\approx 0.8\\%$. We will use a liquiditation time of $T = 60$ days and we will set the number of trades $N = 60$. This means that $\\tau=\\frac{T}{N} = 1$ which means we will be making one trade per day.   \n",
    "股票将具有12%的年度波动率，买卖差价为1/8，平均每日的交易量为500万股。假设一年中有250个交易日，则股票价格每天的波动幅度为$0.12 / \\sqrt{250} \\approx 0.8\\%$。我们将使用$T = 60$天的清算时间，并设置交易数量为$N = 60$。这意味中$\\tau=\\frac{T}{N} = 1$，也就是说我们每天将进行一笔交易  \n",
    "\n",
    "For the temporary cost function we will set the fixed cost of selling to be 1/2 of the bid-ask spread, $\\epsilon = 1/16$. we will set $\\eta$ such that for each one percent of the daily volume we trade, we incur a price impact equal to the bid-ask\n",
    "spread. For example, trading at a rate of $5\\%$ of the daily trading volume incurs a one-time cost on each trade of 5/8. Under this assumption we have $\\eta =(1/8)/(0.01 \\times 5 \\times 10^6) = 2.5 \\times 10^{-6}$.  \n",
    "对于临时成本函数，我们将固定销售成本设置为买卖差价的1/2，$\\epsilon = 1/16$。我们设置$\\eta$，以使我们每天交易量的每1%产生的价格影响等于买卖价差。例如，以每日交易量的$5\\%$的价格进行交易，每次交易的成本为5/8.在此假设下，我们有$\\eta =(1/8)/(0.01 \\times 5 \\times 10^6) = 2.5 \\times 10^{-6}$  \n",
    "\n",
    "For the permanent costs, a common rule of thumb is that price effects become significant when we sell $10\\%$ of the daily volume. If we suppose that significant means that the price depression is one bid-ask spread, and that the effect is linear for smaller and larger trading rates, then we have $\\gamma = (1/8)/(0.1 \\times 5 \\times 10^6) = 2.5 \\times 10^{-7}$.   \n",
    "对于永久性成本，通常的经验法则是，当我们卖出每日交易量的$10\\%$时，价格效应会变得很值得注意。如果我们假设值得注意的意思是价格下跌是一个买卖价差，并且对于越来越小的交易率，其影响是线性的，那么我们 $\\gamma = (1/8)/(0.1 \\times 5 \\times 10^6) = 2.5 \\times 10^{-7}$  \n",
    "\n",
    "The tables below summarize the default parameters of the simulation environment  \n",
    "下表总结了模拟环境的默认参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-19T11:35:38.102332Z",
     "start_time": "2019-11-19T11:35:37.383771Z"
    }
   },
   "outputs": [],
   "source": [
    "import utils\n",
    "\n",
    "# Get the default financial and AC Model parameters  \n",
    "# 获取默认的金融和AC模型参数\n",
    "financial_params, ac_params = utils.get_env_param()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-19T11:35:39.215877Z",
     "start_time": "2019-11-19T11:35:39.210954Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>Financial Parameters</caption>\n",
       "<tr>\n",
       "  <th>Annual Volatility:</th>  <td>12%</td> <th>  Bid-Ask Spread:    </th>     <td>0.125</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Daily Volatility:</th>  <td>0.8%</td> <th>  Daily Trading Volume:</th> <td>5,000,000</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.table.SimpleTable'>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "financial_params\n",
    "# Annual Volatility 年度波动率\n",
    "# Bid-Ask Spread 买卖差价\n",
    "# Daily Volatility 每日波动率\n",
    "# Daily Trading Volume 每日交易量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-19T11:36:15.569676Z",
     "start_time": "2019-11-19T11:36:15.564707Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>Almgren and Chriss Model Parameters</caption>\n",
       "<tr>\n",
       "  <th>Total Number of Shares to Sell:</th>                  <td>1,000,000</td> <th>  Fixed Cost of Selling per Share:</th> <td>$0.062</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Starting Price per Share:</th>                         <td>$50.00</td>   <th>  Trader's Risk Aversion:</th>           <td>1e-06</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Price Impact for Each 1% of Daily Volume Traded:</th> <td>$2.5e-06</td>  <th>  Permanent Impact Constant:</th>       <td>2.5e-07</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Number of Days to Sell All the Shares:</th>              <td>60</td>     <th>  Single Step Variance:</th>             <td>0.144</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Number of Trades:</th>                                   <td>60</td>     <th>  Time Interval between trades:</th>      <td>1.0</td>  \n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.table.SimpleTable'>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ac_params\n",
    "# Total Number of Shares to Sell 出售股票总数\n",
    "# Fixed Cost of Selling per Share 每股固定销售成本\n",
    "# Starting Price per Share 每股起拍价\n",
    "# Trader's Risk Aversion 交易者的风险规避\n",
    "# Price Impact for Each 1% of Daily Volume Traded 每日交易量的1%的价格影响\n",
    "# Permanent Impact Constant 永久影响常数\n",
    "# Number of Days to Sell All the Shares 出售所有股份的天数\n",
    "# Single Step Variance 单步方差\n",
    "# Number of Trades 交易数\n",
    "# Time Interval between trades 交易之间的时间间隔"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning\n",
    "强化学习\n",
    "\n",
    "In the code below we use DDPG to find a policy that can generate optimal trading trajectories that minimize implementation shortfall, and can be benchmarked against the Almgren and Chriss model. We will implement a typical reinforcement learning workflow to train the actor and critic using the simulation environment. We feed the states observed from our simulator to an agent. The Agent first predicts an action using the actor model and performs the action in the environment. Then, environment returns the reward and new state. This process continues for the given number of episodes. To get accurate results, you should run the code at least 10,000 episodes.  \n",
    "在下面的代码中，我们使用DDPG查找可以生成最佳交易轨迹的策略，以最大程度减少实施差额，并可以以Almgren和Chriss模型为基准。我们将实现一个典型的强化学习工作流程，以在模拟环境中训练演员和评论家。我们将从模拟器中观察到的状态反馈给智能体。代理首先用演员模型预测一个动作，然后在环境中执行这个动作。然后环境返回新的状态和奖励。对于给定的周期，这个动作将持续进行。为了获取一个精确的结果，你至少应该跑10000个周期"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-11-19T13:23:46.701Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode [100/10000]\tAverage Shortfall: $2,276,780.07\n",
      "Episode [200/10000]\tAverage Shortfall: $2,562,254.63\n",
      "Episode [300/10000]\tAverage Shortfall: $2,562,500.00\n",
      "Episode [400/10000]\tAverage Shortfall: $2,562,500.00\n",
      "Episode [500/10000]\tAverage Shortfall: $2,562,500.00\n",
      "Episode [600/10000]\tAverage Shortfall: $2,562,500.00\n",
      "Episode [700/10000]\tAverage Shortfall: $2,562,500.00\n",
      "Episode [800/10000]\tAverage Shortfall: $2,562,500.00\n",
      "Episode [900/10000]\tAverage Shortfall: $2,562,500.00\n",
      "Episode [1000/10000]\tAverage Shortfall: $2,562,500.00\n",
      "Episode [1100/10000]\tAverage Shortfall: $2,562,500.00\n",
      "Episode [1200/10000]\tAverage Shortfall: $2,562,500.00\n",
      "Episode [1300/10000]\tAverage Shortfall: $2,562,500.00\n",
      "Episode [1400/10000]\tAverage Shortfall: $2,562,500.00\n",
      "Episode [1500/10000]\tAverage Shortfall: $2,562,500.00\n",
      "Episode [1600/10000]\tAverage Shortfall: $2,562,500.00\n",
      "Episode [1700/10000]\tAverage Shortfall: $2,562,500.00\n",
      "Episode [1800/10000]\tAverage Shortfall: $2,562,500.00\n",
      "Episode [1900/10000]\tAverage Shortfall: $2,562,500.00\n",
      "Episode [2000/10000]\tAverage Shortfall: $2,562,500.00\n",
      "Episode [2100/10000]\tAverage Shortfall: $2,250,908.89\n",
      "Episode [2200/10000]\tAverage Shortfall: $721,204.05\n",
      "Episode [2300/10000]\tAverage Shortfall: $660,355.06\n",
      "Episode [2400/10000]\tAverage Shortfall: $671,785.43\n",
      "Episode [2500/10000]\tAverage Shortfall: $598,614.03\n",
      "Episode [2600/10000]\tAverage Shortfall: $701,206.83\n",
      "Episode [2700/10000]\tAverage Shortfall: $663,251.46\n",
      "Episode [2800/10000]\tAverage Shortfall: $657,303.85\n",
      "Episode [2900/10000]\tAverage Shortfall: $659,449.09\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "import syntheticChrissAlmgren as sca\n",
    "from ddpg_agent import Agent\n",
    "\n",
    "from collections import deque\n",
    "\n",
    "# Create simulation environment\n",
    "# 创建一个模拟环境\n",
    "env = sca.MarketEnvironment()\n",
    "\n",
    "# Initialize Feed-forward DNNs for Actor and Critic models. \n",
    "# 初始化演员批判家模型的前向反馈DNN\n",
    "agent = Agent(state_size=env.observation_space_dimension(), action_size=env.action_space_dimension(), random_seed=0)\n",
    "\n",
    "# Set the liquidation time\n",
    "# 设置清算周期\n",
    "lqt = 60\n",
    "\n",
    "# Set the number of trades\n",
    "# 设置交易数量\n",
    "n_trades = 60\n",
    "\n",
    "# Set trader's risk aversion\n",
    "# 设置交易者的风险规避\n",
    "tr = 1e-6\n",
    "\n",
    "# Set the number of episodes to run the simulation\n",
    "# 设置模拟环境的运行剧集数\n",
    "episodes = 10000\n",
    "\n",
    "shortfall_hist = np.array([])\n",
    "shortfall_deque = deque(maxlen=100)\n",
    "\n",
    "for episode in range(episodes): \n",
    "    # Reset the enviroment\n",
    "    # 重置环境\n",
    "    cur_state = env.reset(seed = episode, liquid_time = lqt, num_trades = n_trades, lamb = tr)\n",
    "\n",
    "    # set the environment to make transactions\n",
    "    # 设置交易环境\n",
    "    env.start_transactions()\n",
    "\n",
    "    for i in range(n_trades + 1):\n",
    "      \n",
    "        # Predict the best action for the current state. \n",
    "        # 预测当前状态的最佳动作\n",
    "        action = agent.act(cur_state, add_noise = True)\n",
    "        \n",
    "        # Action is performed and new state, reward, info are received. \n",
    "        # 执行操作并接收新状态，奖励和信息\n",
    "        new_state, reward, done, info = env.step(action)\n",
    "        \n",
    "        # current state, action, reward, new state are stored in the experience replay\n",
    "        # 当前状态，动作，奖励，新状态存储在经验回放中\n",
    "        agent.step(cur_state, action, reward, new_state, done)\n",
    "        \n",
    "        # roll over new state\n",
    "        # 刷新状态\n",
    "        cur_state = new_state\n",
    "\n",
    "        if info.done:\n",
    "            shortfall_hist = np.append(shortfall_hist, info.implementation_shortfall)\n",
    "            shortfall_deque.append(info.implementation_shortfall)\n",
    "            break\n",
    "        \n",
    "    if (episode + 1) % 100 == 0: # print average shortfall over last 100 episodes\n",
    "        print('\\rEpisode [{}/{}]\\tAverage Shortfall: ${:,.2f}'.format(episode + 1, episodes, np.mean(shortfall_deque)))        \n",
    "\n",
    "print('\\nAverage Implementation Shortfall: ${:,.2f} \\n'.format(np.mean(shortfall_hist)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Todo\n",
    "\n",
    "The above code should provide you with a starting framework for incorporating more complex dynamics into our model. Here are a few things you can try out:  \n",
    "上面的代码应为您提供一个入门框架，以将更复杂的变化纳入我们的模型。 您可以尝试以下几件事：\n",
    "\n",
    "- Incorporate your own reward function in the simulation environmet to see if you can achieve a expected shortfall that is better (lower) than that produced by the Almgren and Chriss model.  \n",
    "将你自己的奖励方法并入模拟环境中，以便观察是否可以实现比Almgren和Chriss模型更好的(更低)的预期缺口\n",
    "\n",
    "\n",
    "- Experiment rewarding the agent at every step and only giving a reward at the end.  \n",
    "尝试给代理每一个奖励或者只在最后给一个奖励\n",
    "\n",
    "\n",
    "- Use more realistic price dynamics, such as geometric brownian motion (GBM). The equations used to model GBM can be found in section 3b of this [paper](https://ro.uow.edu.au/cgi/viewcontent.cgi?referer=https://www.google.com/&httpsredir=1&article=1705&context=aabfj)  \n",
    "使用更真实的价格动态，例如几何布朗运动。用于建模GBM的方程式可以在这个paper的第3b节中找到\n",
    "\n",
    "\n",
    "- Try different functions for the action. You can change the values of the actions produced by the agent by using different functions. You can choose your function depending on the interpretation you give to the action. For example, you could set the action to be a function of the trading rate.  \n",
    "尝试不同的关于动作的方法。你可以使用不同的功能来更改代理产生的动作的值。你可以根据对动作的解释来选择功能。例如，你可以将动作设置为交易汇率的函数\n",
    "\n",
    "\n",
    "- Add more complex dynamics to the environment. Try incorporate trading fees, for example. This can be done by adding and extra term to the fixed cost of selling, $\\epsilon$.  \n",
    "向环境添加更复杂的变化。例如，尝试合并交易费。这可以通过在固定的销售成本$\\epsilon$上加上额外的项来实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
